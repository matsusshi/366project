{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12448771,"sourceType":"datasetVersion","datasetId":7852792}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.675907Z","iopub.status.idle":"2025-07-13T05:50:43.676308Z","shell.execute_reply.started":"2025-07-13T05:50:43.676127Z","shell.execute_reply":"2025-07-13T05:50:43.676142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport cv2\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom torchinfo import summary\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score\nimport PIL\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom collections import OrderedDict\nimport platform\nimport psutil\nimport random\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.transforms import ColorJitter, RandomRotation, RandomResizedCrop\nfrom torchvision.transforms.functional import gaussian_blur\nfrom PIL import ImageOps\nfrom tabulate import tabulate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.676882Z","iopub.status.idle":"2025-07-13T05:50:43.677259Z","shell.execute_reply.started":"2025-07-13T05:50:43.677064Z","shell.execute_reply":"2025-07-13T05:50:43.677079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Old directory (read-only)\nold_dir = \"/kaggle/input/fishdataset/Fish Data/Aair/Raw Data\"\n\n# Temporary copy location (writable)\ncopy_dir = \"/kaggle/working/Raw Data\"\n\n# New renamed directory\nnew_dir = \"/kaggle/working/Raw\"\n\n# Step 1: Remove existing 'Raw' directory if it exists\nif os.path.exists(new_dir):\n    shutil.rmtree(new_dir)  # This removes the directory and everything inside it\n\n# Step 2: Copy the original directory to working space\nif os.path.exists(copy_dir):  # If already copied, skip copy to save time\n    print(\"Copy already exists.\")\nelse:\n    shutil.copytree(old_dir, copy_dir)\n\n# Step 3: Rename the copied directory\nshutil.move(copy_dir, new_dir)  # Better than os.rename for moving/renaming folders\n\n# Step 4: Verify\nprint(\"Directories in /kaggle/working/:\")\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.677782Z","iopub.status.idle":"2025-07-13T05:50:43.678178Z","shell.execute_reply.started":"2025-07-13T05:50:43.677953Z","shell.execute_reply":"2025-07-13T05:50:43.678009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed = 1\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.680094Z","iopub.status.idle":"2025-07-13T05:50:43.680668Z","shell.execute_reply.started":"2025-07-13T05:50:43.680478Z","shell.execute_reply":"2025-07-13T05:50:43.680496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define data directory\ndata_dir = \"/kaggle/input/fishdataset/Fish Data\"\n\n# Collect file paths and labels\nfile_paths = []\nlabels = []\n\nfor class_name in os.listdir(data_dir):\n    class_dir = os.path.join(data_dir, class_name)\n    for subdir in [\"Augmented\", \"Raw\"]:\n        subdir_path = os.path.join(class_dir, subdir)\n        if os.path.isdir(subdir_path):  # Ensure the directory exists\n            for img_file in os.listdir(subdir_path):\n                if img_file.endswith(\".jpg\"):  # Ensure only image files are added\n                    file_paths.append(os.path.join(subdir_path, img_file))\n                    labels.append(class_name)\n\n# Create DataFrame\ndf = pd.DataFrame({\"file_path\": file_paths, \"label\": labels})\ndf = df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\n\n# Split dataset into train, validation, and test sets\ntrain_dataframe, temp_dataframe = train_test_split(df, test_size=0.30, stratify=df['label'], random_state=42)\nvalid_dataframe, test_df = train_test_split(temp_dataframe, test_size=0.50, stratify=temp_dataframe['label'], random_state=42)\n\nprint(\"Training Data:\", len(train_dataframe))\nprint(\"Validation Data:\", len(valid_dataframe))\nprint(\"Test Data:\", len(test_df))\nprint(\"-\")\nprint(\"Total amounts of data in the dataset:\", len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.681563Z","iopub.status.idle":"2025-07-13T05:50:43.681892Z","shell.execute_reply.started":"2025-07-13T05:50:43.681723Z","shell.execute_reply":"2025-07-13T05:50:43.681738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure device is defined\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define save path for checkpoints\nsave_path_checkpoints = \"/kaggle/working/checkpoints\"\n\n# Make sure directory exists\nos.makedirs(save_path_checkpoints, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.682726Z","iopub.status.idle":"2025-07-13T05:50:43.683054Z","shell.execute_reply.started":"2025-07-13T05:50:43.682872Z","shell.execute_reply":"2025-07-13T05:50:43.682887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\n# Data transformations\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.683864Z","iopub.status.idle":"2025-07-13T05:50:43.684197Z","shell.execute_reply.started":"2025-07-13T05:50:43.684065Z","shell.execute_reply":"2025-07-13T05:50:43.684081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\n\nclass FishDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n        # Create a mapping from class name to integer index\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(sorted(dataframe['label'].unique()))}\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['file_path']\n        label_name = self.dataframe.iloc[idx]['label']\n        label = self.class_to_idx[label_name]  # Convert label to integer index\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.685426Z","iopub.status.idle":"2025-07-13T05:50:43.685903Z","shell.execute_reply.started":"2025-07-13T05:50:43.685758Z","shell.execute_reply":"2025-07-13T05:50:43.685771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_batch = 32\nval_batch = 8\n# Create datasets\ntrain_dataset = FishDataset(train_dataframe, transform=train_transform)\nvalid_dataset = FishDataset(valid_dataframe, transform=val_transform)\ntest_dataset = FishDataset(test_df, transform=val_transform)\n\n# Create dataloaders\ndataloader_train_dataset = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\ndataloader_valid_dataset  = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\ndataloader_test_dataset = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.687190Z","iopub.status.idle":"2025-07-13T05:50:43.687619Z","shell.execute_reply.started":"2025-07-13T05:50:43.687351Z","shell.execute_reply":"2025-07-13T05:50:43.687365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load pretrained ResNet50\nmodel = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n\n# Replace the final fully connected layer\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 20)  # Assuming 20 fish types\n\n# Move model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Define loss function and optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\nsummary(model, input_size=(train_batch, 3, 224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.688574Z","iopub.status.idle":"2025-07-13T05:50:43.688860Z","shell.execute_reply.started":"2025-07-13T05:50:43.688703Z","shell.execute_reply":"2025-07-13T05:50:43.688717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport os\nimport pandas as pd\nimport torch\nfrom tqdm.notebook import tqdm\n\ndef train_model(model, criterion, optimizer, dataloader_train_dataset, dataloader_valid_dataset, num_epochs=20, early_stop_patience=5, save_path_checkpoints=\"checkpoints\"):\n    train_loss_history = []\n    train_acc_history = []\n    val_loss_history = []\n    val_acc_history = []\n\n    best_val_acc = 0.0\n    consecutive_no_improvement = 0\n    num_epochs_loss_greater = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        progress_bar = tqdm(enumerate(dataloader_train_dataset), total=len(dataloader_train_dataset))\n        for i, (inputs, labels) in progress_bar:  # Only unpack 2 values\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            _, predicted = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n\n            progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n            progress_bar.set_postfix(loss=running_loss / total_train, acc=correct_train / total_train)\n\n        epoch_train_loss = running_loss / total_train\n        epoch_train_acc = correct_train / total_train\n        train_loss_history.append(epoch_train_loss)\n        train_acc_history.append(epoch_train_acc)\n        print('Training Loss: {:.3f} Acc: {:.3f}'.format(epoch_train_loss, epoch_train_acc))\n\n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        with torch.no_grad():\n            for inputs, labels in dataloader_valid_dataset:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                _, predicted = torch.max(outputs, 1)\n                running_loss += loss.item() * inputs.size(0)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n\n        epoch_val_loss = running_loss / total_val\n        epoch_val_acc = correct_val / total_val\n        val_loss_history.append(epoch_val_loss)\n        val_acc_history.append(epoch_val_acc)\n        print('Validation Loss: {:.3f} Acc: {:.3f}'.format(epoch_val_loss, epoch_val_acc))\n\n        # Early stopping logic\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            best_epoch = epoch + 1\n            filepath = f\"{save_path_checkpoints}/model.pt\"\n            checkpoint = {\n                \"epoch\": epoch + 1,\n                \"model_weight\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict()\n            }\n            torch.save(checkpoint, filepath)\n            print(f\"Best model saved at epoch {best_epoch} with validation accuracy: {best_val_acc:.3f}\")\n            consecutive_no_improvement = 0\n        else:\n            consecutive_no_improvement += 1\n\n        if epoch_val_loss > epoch_train_loss:\n            num_epochs_loss_greater += 1\n        else:\n            num_epochs_loss_greater = 0\n\n        if consecutive_no_improvement >= early_stop_patience or num_epochs_loss_greater >= early_stop_patience:\n            print(f\"Early stopping criteria met. Training stopped at epoch {epoch + 1}.\")\n            break\n\n    return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n\n\n# Start training\nstart_time = time.time()\ntrain_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(\n    model, criterion, optimizer, dataloader_train_dataset, dataloader_valid_dataset,\n    save_path_checkpoints=save_path_checkpoints\n)\nend_time = time.time()\ntraining_time = end_time - start_time\nprint(f\"Training Time: {training_time:.2f} seconds ---> {training_time/60:.2f} minutes\")\n\n# Save training history\ndata = {\n    'Epoch': list(range(1, len(train_loss_history) + 1)),\n    'Train Loss': train_loss_history,\n    'Train Accuracy': train_acc_history,\n    'Validation Loss': val_loss_history,\n    'Validation Accuracy': val_acc_history\n}\nhistory = pd.DataFrame(data)\nhistory.to_excel('/kaggle/working/training_data.xlsx', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T05:50:43.690238Z","iopub.status.idle":"2025-07-13T05:50:43.690595Z","shell.execute_reply.started":"2025-07-13T05:50:43.690370Z","shell.execute_reply":"2025-07-13T05:50:43.690384Z"}},"outputs":[],"execution_count":null}]}